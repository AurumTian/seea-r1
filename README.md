# SEAR-R1

## ðŸ“¢ Latest News
* [2025.03.31] Implemented continuous MCTS sampling and GRPO training
* [2025.03.23] Implemented multi-threaded data collection
* [2025.03.21] Integrated MCTS data collection and GRPO training
* [2025.03.19] ALFWorld integrated into MCTS
* [2025.03.14] Integrated ms-swift training framework, supporting one-click data generation and instruction fine-tuning (SFT)
* [2025.03.11] Integrated ALFWorld simulation environment for data generation
* [2025.02.25] Implemented streaming text generation and parallel tool calls based on FunctionCallAgent
* [2025.01.16] World model based on video generation model integrated into MCTS
* [2025.01.06] Added MCTS (Monte Carlo Tree Search): Uses multi-agents for iterative Selection, Expansion, Simulation, and Back-Propagation, increasing path exploration space and improving task success rates
* [2025.01.06] Added Orchestrator: multi-agents framework
* [2024.12.11] Added VisualAgent with Qwen2-VL support
* [2024.12.06] Added VisualAgent integrating multi-modal perception, task planning, and tool calling

## Features
Currently supported skills:
- VisualAgent: Based on multi-modal large models (GPT-4o/Qwen2-VL), integrates multi-modal perception, task planning, and tool calling
  - perception: Obtain images for perception
  - control: Control
  - interaction: Interaction
- Orchestrator: Multi-agents framework, allowing modification of `stage_to_agent_map` and `stage_transition_graph` to build different multi-agents
  - `stage_to_agent_map`: Different states correspond to different agents
  - `stage_transition_graph`: Transition relationships between states
  - Actor: Generates N actions based on historical information
  - Critic: Ranks the N actions generated by the actor
  - Executor: Executes the selected action
  - Score: Judges the status of the current task execution
  - Dreamer (not yet added): Imagines the environment state based on actions
- MCTS: MCTS (Monte Carlo Tree Search) based on multi-agents, increasing path exploration space and improving task success rates
  - Selection: Starting from the root node, select the child node with the largest UCT value
  - Expansion: Expand leaf nodes, generating N different possible actions and corresponding child nodes
  - Simulation: Perform roll-out on the current node, with options for real interaction, simulation, and dreamer modes (currently only real interaction is implemented)
  - Back-Propagation: Update the Value and UCT of all tree nodes based on the task execution status

## TODO List

- [x] Support local vLLM deployment of Qwen model
- [x] Use ReAct thought chain for accelerated inference and continuous execution
- [x] Add common tools
- [x] Implement multi-modal Agent based on GPT4o
- [x] Multi-modal Agent supports calling open-source multi-modal large models like Qwen-VL
- [ ] Integrate simulation testing based on open-source embodied intelligence Benchmarks
  - [x] Integrate ALFWorld Benchmark
  - [ ] Integrate ALFRED Benchmark
  - [ ] Integrate Habitat Benchmark
  - [ ] Integrate VisualAgentBench Benchmark
- [x] Implement data generation and instruction fine-tuning
  - [x] Unify data format to ReAct-based multi-turn multi-modal interaction data format
  - [x] Random sampling data generation based on ALFWorld simulation environment
  - [x] Random sampling data generation based on Kuaishou Keling video generation large model as world model
  - [x] MCTS search-based data generation based on Kuaishou Keling video generation large model as world model
  - [x] Integrate ms-swift for SFT
  - [x] Integrate ms-swift for GRPO
  - [x] Implement multi-turn multi-modal GRPO
  - [x] Implement multi-threaded sampling
  - [x] Implement continuous online sampling and instruction fine-tuning (GRPO)
- [ ] Reproduce AgentQ's MCTS + DPO + self critique
  - [x] Implement multi-agents framework orchestrator
  - [x] Implement MCTS based on multi-agents
  - [x] Generate SFT data based on MCTS success paths
  - [x] Generate GRPO data based on MCTS success paths
  - [ ] Generate DPO data based on MCTS failed and successful paths
  - [ ] Generate error-reflection-success reflection reasoning data based on MCTS failed and successful paths
  - [ ] Generate PPO and other RL data based on MCTS
  - [x] Introduce Dreamer for exploration
  - [ ] Integrate RoboMind dataset for large-scale data generation

## Installation Instructions

### Using Recursive Clone
Recursive clone of the repository (including all submodules)
```bash
git clone --recursive https://github.com/your-username/seea_r1.git
cd seea_r1
```

Create a virtual environment (recommended)
```bash
conda create -n seea_r1 python=3.12
conda activate seea_r1
```

Run installation script
```bash
pip install -r requirements.txt
```

Download PDDL & Game Files and pre-trained MaskRCNN detector:
```bash
export ALFWORLD_DATA=<storage_path>
python scripts/alfworld-download
```

### Updating Submodules
If you have already cloned the repository but did not use the --recursive option, you can execute the following command:

```bash
# Initialize submodules
git submodule update --init --recursive
```


### Notes
1. After using submodules, others who clone your repository need to use the --recursive option, or execute git submodule update --init --recursive after cloning
2. Submodules will be locked to a specific commit, if you need to update the submodules to the latest version, you need to enter the submodule directory and execute git pull
3. -e option means editable mode installation, so you don't need to reinstall after modifying the submodule code during development


## Usage
### Configuration File Usage
SEEA-R1 supports using YAML configuration files to manage complex parameter settings, avoiding the need to input a lot of command line parameters each time.
#### Generate Default Configuration File
```bash
python main.py --generate-config configs/default_config.yaml
```
#### Use Configuration File to Run
```bash
python main.py --config configs/example_config.yaml
```
#### Mixing Configuration File and Command Line Parameters
Command line parameters take precedence over settings in the configuration file:
```bash
python main.py --config configs/example_config.yaml --num 5 --temperature 0.7
```
#### Configuration File Example
```yaml
# Basic Configuration
version: "mcts"  # Optional: mcts, visual
model: "Qwen2_5-VL-72B-Instruct"
dataset_root: "/path/to/dataset"
num: 10  # Task Quantity
save_dir: "./samples"

# Environment Configuration
alfworld_env: true  # Whether to use ALFWorld environment
visual_perception: false
wo_image_tool_result: false

# Model Generation Parameters
temperature: 0.6
top_p: 0.9
repetition_penalty: 1.05
max_tokens: 2048
image_size: [600, 600]

# Parallel Configuration
threads: 4  # Parallel Thread Count, 0 means automatic selection
thread_agent: true  # Create separate agent instances for each thread

# Data Collection Configuration
max_valid_data: 100  # Maximum Effective Data Quantity, -1 means no limit
sort_action: false  # Whether to sort actions

# MCTS Specific Parameters
mcts:
  num_proposed_action: 5
  n_iterations: 10
  depth_limit: 30
  exploration_weight: 1.0
 ```

### Inference & Sampling
#### Test Based on ALFWorld Simulation Environment
```bash
python main.py --version visual --model Qwen2_5-VL-32B-Instruct --alfworld_env
```

#### Random Sampling Based on World Model
```bash
python main.py --version visual --model Qwen2_5-VL-72B-Instruct
```

#### Use MCTS for Sampling Search
```bash
python main.py --version mcts --model Qwen2_5-VL-72B-Instruct-AWQ
```

#### Use MCTS, Explore in ALFWorld Simulation Environment in Multi-modal Mode
```bash
python main.py --version mcts --model Qwen2_5-VL-72B-Instruct-AWQ --alfworld_env 
```

#### Use MCTS, Explore in ALFWorld Simulation Environment in Multi-modal Mode
```bash
python main.py --version mcts --model Qwen2_5-VL-72B-Instruct --alfworld_env --visual_perception --temperature 0.6 --top_p 0.95 --repetition_penalty 1.05
```

#### Multi-threaded Sampling
```bash
python main.py --version mcts --model Qwen2_5-VL-72B-Instruct --alfworld_env --num 64 --threads 4 --thread-agent
```

#### Run on Remote Server
```bash
sudo apt-get update && sudo apt-get install -y \
    xvfb \
    x11-xserver-utils \
    libxrender1 \
    libgl1-mesa-glx \
    xserver-xorg-video-dummy

xvfb-run -a python main.py --version mcts --model Qwen2_5-VL-72B-Instruct --alfworld_env --temperature 0.9 --top_p 0.95 --repetition_penalty 1.05
```
### Evaluation
SEEA-R1 provides comprehensive evaluation functionality, supporting performance evaluation on different models and configurations in ALFWorld environment.
#### Command Line Parameters

Besides using configuration files, you can also use the following command line parameters to control the evaluation process:

```bash
python -m seea.eval.evaluate [parameters]
```
Main parameters description:

- --model : Use predefined model configuration name, such as "gpt-4", "Qwen2_5-72B-Instruct", etc
- --model_name : Directly specify model name, with higher priority than --model parameter
- --base_url : Specify the base URL of the model API, used with --model_name
- --split : Data set division ["train", "dev", "test"]
- --num_games : Evaluation task quantity
- --repeats : Number of repetitions for each task
- --max_steps : Maximum steps for each episode
- --max_workers : Number of parallel tasks

### Example
1. Use predefined model configuration:
```bash
python -m seea.eval.evaluate --model gpt-4 --split dev
 ```
```

2. Use custom model and API address:
```bash
python -m seea.eval.evaluate --model_name Qwen2_5-72B-Instruct --base_url http://your-api-endpoint --split dev
 ```
```

3. Use configuration file:
```bash
python -m seea.eval.evaluate --config configs/eval_config_llm.yaml
 ```
#### Multi-modal Evaluation
```bash
python -m seea.eval.evaluate --config configs/eval_config.yaml
```
#### Evaluation Configuration File Example
```yaml
# Evaluation Basic Configuration
version: "visual"  # Optional: mcts, visual
model: "Qwen2_5-VL-72B-Instruct"
repeats: 1  # Number of repetitions for each task
output_dir: "eval_results"  # Result Output Directory
log_level: "INFO"  # Log Level
save_intermediate: true  # Whether to save intermediate results
num_games: 20  # Evaluation task quantity
split: "dev"  # Data set division: train/dev/test
debug: false  # Whether to enable debug mode
max_steps: 50  # Maximum steps for each episode
max_workers: 4  # Number of parallel tasks

# Model Generation Parameters
temperature: 0.6
top_p: 0.9
repetition_penalty: 1.05
max_tokens: 2048
image_size: [600, 600]
```
#### Multi-threaded Parallel Evaluation
```bash
python -m seea.eval.evaluate --version visual --model Qwen2_5-VL-72B-Instruct --split dev --num_games 100 --max_workers 8
```
#### Evaluation Result Analysis
Evaluation results will be saved in the specified output directory, including the following indicators:

- Success Rate: Proportion of tasks completed
- Average Steps: Average steps required to complete tasks
- Average Points: Average score of task completion
- Average Goal Condition Points: Average score of goal condition completion 

## Training
### Train Based on ALFWorld Simulation Environment
#### Online Sampling and Training
GRPO training script and input parameters
```bash
./seea/train/run_pipeline.sh <Training Type> <Training Method> <Model Path> [Save Directory] [Dataset Path] [Use delta_reward (Only Effective for DPO)]
```

Training type optional values:
- GRPO: Use GRPO algorithm for training
- DPO: Use DPO algorithm for training

Training method optional values:
- full: Complete model training
- lora: LoRA parameter efficient fine-tuning (model will be automatically merged after training)

delta_reward parameter (Only Effective for DPO):
- true: Use DPO data with delta_reward
- false or not specified: Use normal DPO data

GRPO training script example (Complete Training):
```
nohup ./seea/train/run_pipeline.sh GRPO full /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct  2>&1 & disown
```

GRPO training script example (LoRA Training):
```
nohup ./seea/train/run_pipeline.sh GRPO lora /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct  2>&1 & disown
```

DPO training script example (Using delta_reward):
```
nohup ./seea/train/run_pipeline.sh DPO full /media/users/name/models/Qwen/Qwen2.5-7B-Instruct /media/users/name/seea/Qwen2.5-7B-Instruct /path/to/dataset true 2>&1 & disown
```

#### nohup for Background Training

1. Specify Model (Complete Training):
```bash
nohup ./seea/train/run_pipeline.sh GRPO full /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct  2>&1 & disown
```

2. Specify Model (LoRA Training):
```bash
nohup ./seea/train/run_pipeline.sh GRPO lora /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct  2>&1 & disown
```

3. Specify Model, Save Directory, and Dataset Path (Complete Training):
```bash
nohup ./seea/train/run_pipeline.sh GRPO full /media/users/qwen/Qwen2.5-7B-Instruct /media/users/name/seea  /root/name/seea/samples/mcts/Qwen2.5-7B-Instruct/alfworld/2025-03-29_16-36-58/dataset_filtered.json   2>&1 & disown
```

4. Specify Model, Save Directory, and Dataset Path (LoRA Training):
```bash
nohup ./seea/train/run_pipeline.sh GRPO lora /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct  /media/users/name/seea  /root/name/seea/samples/mcts/Qwen2.5-7B-Instruct/alfworld/2025-03-29_16-36-58/dataset_filtered.json   2>&1 & disown
```

5. Specify Model, Save Directory, Dataset Path, and Use delta_reward (DPO Training):
```bash
nohup ./seea/train/run_pipeline.sh DPO lora /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct /media/users/name/seea/Qwen2.5-VL-7B-Instruct /root/name/seea/samples/mcts/Qwen2.5-7B-Instruct/alfworld/2025-03-29_16-36-58/dataset_filtered.json true 2>&1 & disown
```

6. Use delta_reward but do not specify dataset (DPO Training):
```bash
nohup ./seea/train/run_pipeline.sh DPO full /media/users/name/models/Qwen/Qwen2.5-7B-Instruct /media/users/name/seea/Qwen2.5-7B-Instruct "" true 2>&1 & disown
```

#### Use screen for Background Training
Using screen can continue running processes even after disconnecting. The following steps show how to use screen for training:

1. Start a new screen session:
   
   ```bash
   screen -S seea_training
    ```
2. Run training script in screen session (Complete Training):
   
   ```bash
   ./seea/train/run_pipeline.sh GRPO full /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct
    ```
3. Run training script in screen session (LoRA Training):
   
   ```bash
   ./seea/train/run_pipeline.sh GRPO lora /media/users/name/models/Qwen/Qwen2.5-VL-7B-Instruct
    ```

### Train Based on World Model (TODO)
### Mixed Environment Sampling (TODO)

## Code Merge
Merge request through merge request into master or dev, do not directly push



## TODO
- Enable CICD, Open Integrated Test
